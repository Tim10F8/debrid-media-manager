# TITLE MATCHING BUSINESS LOGIC FLOW
# How the system determines if a test title matches a target title

## INPUT DATA
- **Target Title**: The title we're searching for (e.g., "The Matrix")
- **Years**: Array of years associated with the target (e.g., ["1999"])  
- **Test Title**: The title we're checking (e.g., "The.Matrix.1999.1080p.BluRay.x264-RARBG")

## MAIN DECISION FLOW

### STEP 1: INITIAL CLEANUP
**Purpose**: Remove noise from torrent/file names

1. Remove website prefixes from test title
   - Pattern: "www.site.com - Title" → "Title"
   - Only removes from beginning of string

2. Remove bracketed tags from test title
   - Pattern: "[RARBG] Title" → "Title"
   - Only removes from beginning of string

### STEP 2: PARSE TORRENT METADATA
**Purpose**: Extract clean title from torrent naming conventions

1. Parse test title to separate:
   - Clean title (e.g., "The Matrix")
   - Release group (e.g., "RARBG")
   - Technical specs (resolution, codec, etc.)

### STEP 3: QUICK SIMILARITY CHECK
**Purpose**: Fast path for nearly identical titles

1. Calculate character-level similarity between target and parsed title
2. Use Levenshtein distance (minimum edits needed)
3. Convert to similarity ratio: 1 - (edits / total_characters)

**Decision Point**: 
- If similarity >= 85% → **MATCH FOUND** ✓
- Otherwise → Continue to Step 4

### STEP 4: ALPHANUMERIC COMPARISON
**Purpose**: Handle special character variations

1. Remove ALL special characters from both titles
   - Keep only: letters, numbers, spaces
   - "Spider-Man: No Way Home" → "SpiderMan No Way Home"

2. Check if cleaned target contains cleaned test as substring

**Decision Point**:
- If target contains test → **MATCH FOUND** ✓
- Otherwise → Continue to Step 5

### STEP 5: REMOVE RELEASE GROUP
**Purpose**: Prevent false negatives from group tags

1. If release group was identified in Step 2
2. Remove it from test title completely
   - "The Matrix YIFY" → "The Matrix"

### STEP 6: YEAR VALIDATION
**Purpose**: Ensure correct media year

1. Extract all 4-digit years from test title (1890-2025 range)
2. Filter out year 1920 (common false positive)

**Decision Point**:
- If BOTH test and target have years:
  - Check if any test year matches target years (±1 year tolerance)
  - If NO matches → **NOT A MATCH** ✗
- If either has no years → Continue (permissive)

### STEP 7: DETAILED TITLE MATCHING
**Purpose**: Sophisticated matching with multiple strategies

#### 7A: FLEXIBLE EQUALITY CHECK
Multiple normalization attempts in order:

1. **Try "naked" comparison**:
   - Strip everything except letters/numbers
   - Minimum length: 5 chars (or 3 if year present)
   - Check if test contains target
   
2. **Try without repeated characters**:
   - "Goood" → "God"
   - Check if test contains target

3. **Try without accent marks**:
   - "Amélie" → "Amelie"
   - Check if test contains target

4. **Try direct substring** (if long enough):
   - Requires 8+ characters (or 5+ if year present)
   - Check if test contains target

5. **Try video filename parsing**:
   - Parse as movie/TV show filename
   - Compare extracted titles

**Sub-Decision**: If any flexible equality matches:
- Check if words appear in correct sequence
- Tolerance: up to 5 words between expected terms

**Decision Point**:
- If match AND (has year OR valid sequence) → **MATCH FOUND** ✓
- Otherwise → Continue to 7B

#### 7B: TERM COUNT VALIDATION
**Purpose**: Prevent matching on too-generic titles

1. Split target into individual words
2. Count total terms

**Decision Point**:
- If NO terms → **NOT A MATCH** ✗
- If ≤2 terms AND test has no year → **NOT A MATCH** ✗
- Otherwise → Continue to 7C

#### 7C: TERM CLASSIFICATION
**Purpose**: Identify important vs common words

1. **Key Terms** (important words):
   - Words NOT in common English dictionary
   - OR words longer than 5 characters
   - OR non-alphabetic symbols longer than 2 characters
   - Examples: "Matrix", "Godzilla", "://"

2. **Common Terms** (generic words):
   - Everything else
   - Examples: "The", "of", "and"

**Decision Point**:
- If NO key terms AND ≤2 total terms AND no year → **NOT A MATCH** ✗
- Otherwise → Continue to 7D

#### 7D: SCORING SYSTEM
**Purpose**: Calculate match confidence

1. **Assign point values**:
   - Each key term found: 2 points
   - Each common term found: 1 point
   - Year presence bonus: +40% of total terms

2. **Calculate maximum possible score**:
   - Total = (key_terms × 2) + (common_terms × 1)

3. **Count actual matches**:
   - Search for each term in test title
   - For key terms also try:
     - Without accents (if ≥3 chars)
     - Without repeated chars (if ≥3 chars)  
     - Alphanumeric only (if ≥3 chars)
     - Roman numeral conversion (e.g., "III" → "3")

4. **Calculate final score**:
   - Score = (found_key_terms × 2) + (found_common_terms × 1)
   - If year matches: Score += (total_terms × 0.4)

**Final Decision**:
- If Score >= 80% of maximum → **MATCH FOUND** ✓
- Otherwise → **NOT A MATCH** ✗

---

## SUMMARY OF KEY THRESHOLDS

| Check | Threshold | Purpose |
|-------|-----------|---------|
| Levenshtein Similarity | ≥85% | Fast approval for nearly identical titles |
| Minimum Term Length | 3-5 chars | Context-dependent minimum for matching |
| Year Tolerance | ±1 year | Accounts for production/release year differences |
| Minimum Terms | >2 or has year | Prevents false positives on generic titles |
| Final Score | ≥80% | Overall confidence threshold |
| Sequence Gap Tolerance | 5 words | Maximum distance between expected sequential terms |

## MATCHING STRATEGIES (IN ORDER)

1. **Exact Similarity** - 85% character match
2. **Normalized Substring** - Alphanumeric only
3. **Flexible Equality** - Multiple normalization attempts
4. **Term-Based Scoring** - Weighted word matching

## SPECIAL CASES HANDLED

1. **Torrent Artifacts**: Website prefixes, release groups, technical specs
2. **Character Variations**: Accents, special chars, repeated letters
3. **Roman Numerals**: "Rocky III" matches "Rocky 3"
4. **Year Flexibility**: ±1 year tolerance for production/release differences
5. **Short Titles**: Requires year for confidence on ≤2 word titles
6. **Common Words**: De-weighted in scoring to focus on unique terms
7. **Unicode Support**: Handles multiple character sets (Latin, Cyrillic, Greek, etc.)

## EXAMPLES

### Example 1: "The Matrix" (1999)
Test: "The.Matrix.1999.1080p.BluRay.x264-RARBG"
1. Clean: Remove none (no prefix/brackets)
2. Parse: Title="The Matrix", Group="RARBG"
3. Similarity: High match on "The Matrix"
4. **Result: MATCH** ✓ (Step 3)

### Example 2: "Spider-Man: No Way Home" (2021)
Test: "Spider.Man.No.Way.Home.2021.WEBRip"
1. Clean: Remove none
2. Parse: Title="Spider Man No Way Home"
3. Similarity: ~80% (below threshold)
4. Alphanumeric: "SpiderMan No Way Home" matches
5. **Result: MATCH** ✓ (Step 4)

### Example 3: "It" (2017)
Test: "It.2017.Horror.BluRay"
1. Clean: Remove none
2. Parse: Title="It"
3. Similarity: 100% but only 2 chars
4. Alphanumeric: "It" = "It"
5. Year validation: 2017 present
6. Detailed: Only 1 term, but year present
7. Scoring: "It" found + year bonus
8. **Result: MATCH** ✓ (Step 7D - saved by year)

### Example 4: "The Thing" (1982) vs "The Thing" (2011)
Test: "The.Thing.2011.BluRay"
Target Year: 1982
1. Clean: Remove none
2. Parse: Title="The Thing"
3. Similarity: 100% match
4. **Result: MATCH** ✓ (Step 3)
5. Year Check: 2011 ≠ 1982±1
6. **Result: NO MATCH** ✗ (Step 6 - wrong year)

## BUSINESS RULES SUMMARY

1. **Similarity wins**: 85% character similarity is automatic match
2. **Years matter**: Wrong year can veto an otherwise perfect match
3. **Short titles need context**: ≤2 words require year for confidence
4. **Unique words weighted higher**: "Inception" worth more than "The"
5. **Multiple fallbacks**: Try increasingly aggressive normalizations
6. **Sequence matters**: Words should appear in roughly the right order
7. **80% confidence required**: Final scoring needs 4/5 points minimum